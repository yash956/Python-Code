{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(model2):\n",
    "    smote=SMOTE(sampling_strategy='minority',random_state=42)\n",
    "    accuracy=[]\n",
    "    precision=[]\n",
    "    recall=[]\n",
    "    f1_test=[]\n",
    "    f1_train=[]\n",
    "    l=1176\n",
    "\n",
    "    for i in range(100):\n",
    "        train=resample(new_df.values,n_samples=l)\n",
    "        #print(train[0])\n",
    "        tv=train.tolist()\n",
    "        test=resample(np.array([i for i in new_df.values if i.tolist() not in tv]),\n",
    "                      n_samples=(new_df.shape[0]-l))\n",
    "        #print(test[0])\n",
    "        X_sm, y_sm=smote.fit_sample(train[:,:-1],train[:,-1])\n",
    "        model2.fit(X_sm,y_sm)\n",
    "    \n",
    "        #Appending Scores\n",
    "    \n",
    "        accuracy.append(metrics.accuracy_score(test[:,-1],model2.predict(test[:,:-1])))\n",
    "        precision.append((metrics.precision_score(test[:,-1],model2.predict(test[:,:-1]))))\n",
    "        recall.append((metrics.recall_score(test[:,-1],model2.predict(test[:,:-1]))))\n",
    "        f1_test.append((metrics.f1_score(test[:,-1],model2.predict(test[:,:-1]))))\n",
    "        f1_train.append((metrics.f1_score(y_sm,model2.predict(X_sm))))\n",
    "    return(np.mean(accuracy).round(2),np.mean(precision).round(2),np.mean(recall).round(2),np.mean(f1_test).round(2))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The above function can be used when we have imbalanced data. The logic behind the code is manually split the data into train and test datasets. Then apply sampling technique on train data and test on the testing set. The motive behing this function was to prevent using Kfold cross validation where smote is applied on the entire data and hence split into train and test data.And chances maybe our model will not perform well when we use real life test data to predict the model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for manual Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def One_User_Out(data,model):\n",
    "    '''  \n",
    "    Input: dataframe , Model object\n",
    "    \n",
    "    Returns: Train_score,Test_score\n",
    "    '''\n",
    "    sc=StandardScaler()\n",
    "    user=data.User.unique()\n",
    "    test_score=[]\n",
    "    train_score=[]\n",
    "    for i in user:\n",
    "        X_train=data[data.User!=i].drop(columns=['User','Class'])\n",
    "        y_train=data[data.User!=i].Class\n",
    "        X_test=data[data.User==i].drop(columns=['User','Class'])\n",
    "        y_test=data[data.User==i].Class\n",
    "        \n",
    "        X_train_scaled=sc.fit_transform(X_train)\n",
    "        X_test_scaled=sc.transform(X_test)\n",
    "        \n",
    "        model.fit(X_train_scaled,y_train)\n",
    "        train_score.append(f1_score(y_train,model.predict(X_train_scaled),average='weighted'))\n",
    "        \n",
    "        test_score.append(f1_score(y_test,model.predict(X_test_scaled),average='weighted'))\n",
    "    return(train_score,test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_user_out_comb(model,data):\n",
    "    from itertools import combinations\n",
    "    from sklearn.metrics import f1_score\n",
    "    import random  \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc=StandardScaler()\n",
    "    test_score=[]\n",
    "    train_score=[]\n",
    "    comb=combinations([0,1,2,5,6,8,9,10,11,12,13,14],3)\n",
    "    lst=[]\n",
    "    for i in comb:\n",
    "        lst.append(list(i))\n",
    "    np.random.shuffle(lst)\n",
    "    l1=lst[:35]\n",
    "    for i in l1:\n",
    "        X_train=data[(data.User!=i[0])&(data.User!=i[1])&(data.User!=i[2])].drop(columns=['User','Class'])\n",
    "        y_train=data[(data.User!=i[0])&(data.User!=i[1])&(data.User!=i[2])].Class\n",
    "        X_test=data[(data.User==i[0])|(data.User==i[1])|(data.User==i[2])].drop(columns=['User','Class'])\n",
    "        y_test=data[(data.User==i[0])|(data.User==i[1])|(data.User==i[2])].Class\n",
    "        \n",
    "        X_train_scaled=sc.fit_transform(X_train)\n",
    "        X_test_scaled=sc.transform(X_test)\n",
    "        model.fit(X_train_scaled,y_train)\n",
    "        train_score.append(f1_score(y_train,model.predict(X_train_scaled),average='weighted'))\n",
    "        test_score.append(f1_score(y_test,model.predict(X_test_scaled),average='weighted'))\n",
    "    return(train_score,test_score)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The above two codes are for manual train and test split when we have different users in our target column and we have to classify them as.\n",
    "The One_User_Out  function will keep one user for testing and train on other users.\n",
    "The Three_User_Out_Comb function will keep three users for testing and train on other users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
